{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d522a9d",
   "metadata": {},
   "source": [
    "## Test MLxtend Apriori on Bread/Milk"
   ]
  },
  {
   "cell_type": "code",
   "id": "fc344f9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:42:17.311380Z",
     "start_time": "2025-11-19T10:42:17.306708Z"
    }
   },
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "a48134a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:42:17.342002Z",
     "start_time": "2025-11-19T10:42:17.327442Z"
    }
   },
   "source": [
    "transactions = [\n",
    "    [\"Bread\", \"Milk\"],\n",
    "    [\"Bread\", \"Diapers\", \"Beer\", \"Eggs\"],\n",
    "    [\"Milk\", \"Diapers\", \"Beer\", \"Coke\"],\n",
    "    [\"Bread\", \"Milk\", \"Diapers\", \"Beer\"],\n",
    "    [\"Bread\", \"Milk\", \"Diapers\", \"Coke\"],\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "\n",
    "df_1 = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df_1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Beer  Bread   Coke  Diapers   Eggs   Milk\n",
       "0  False   True  False    False  False   True\n",
       "1   True   True  False     True   True  False\n",
       "2   True  False   True     True  False   True\n",
       "3   True   True  False     True  False   True\n",
       "4  False   True   True     True  False   True"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beer</th>\n",
       "      <th>Bread</th>\n",
       "      <th>Coke</th>\n",
       "      <th>Diapers</th>\n",
       "      <th>Eggs</th>\n",
       "      <th>Milk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a7647a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:42:17.426089Z",
     "start_time": "2025-11-19T10:42:17.411131Z"
    }
   },
   "source": [
    "res = apriori(df_1, min_support=0.6, use_colnames=True)\n",
    "print(res)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   support          itemsets\n",
      "0      0.6            (Beer)\n",
      "1      0.8           (Bread)\n",
      "2      0.8         (Diapers)\n",
      "3      0.8            (Milk)\n",
      "4      0.6   (Diapers, Beer)\n",
      "5      0.6  (Diapers, Bread)\n",
      "6      0.6     (Milk, Bread)\n",
      "7      0.6   (Diapers, Milk)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "a1e924e0",
   "metadata": {},
   "source": [
    "## Test Spark MLlib Apriori on Bread/Milk"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:42:17.533484Z",
     "start_time": "2025-11-19T10:42:17.524209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_2 = pd.DataFrame({\n",
    "    'items': transactions\n",
    "})\n",
    "df_2"
   ],
   "id": "a0ed649df63249c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                          items\n",
       "0                 [Bread, Milk]\n",
       "1  [Bread, Diapers, Beer, Eggs]\n",
       "2   [Milk, Diapers, Beer, Coke]\n",
       "3  [Bread, Milk, Diapers, Beer]\n",
       "4  [Bread, Milk, Diapers, Coke]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Bread, Milk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Bread, Diapers, Beer, Eggs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Milk, Diapers, Beer, Coke]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Bread, Milk, Diapers, Beer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Bread, Milk, Diapers, Coke]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:42:20.390129Z",
     "start_time": "2025-11-19T10:42:17.647348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FPGrowthExample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.userClassPathFirst\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "fp = FPGrowth(\n",
    "    itemsCol=\"items\",\n",
    "    minSupport=0.6,\n",
    "    minConfidence=0.5\n",
    ")\n",
    "\n",
    "model = fp.fit(df_2)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "model.freqItemsets.show(truncate=False)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5. Display association rules\n",
    "# --------------------------------------------\n",
    "print(\"Association Rules:\")\n",
    "model.associationRules.show(truncate=False)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6. Display predictions (items likely to be added)\n",
    "# --------------------------------------------\n",
    "print(\"Predictions:\")\n",
    "model.transform(df_2).show(truncate=False)\n"
   ],
   "id": "42d12c78edb5277d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/19 11:42:19 WARN Utils: Your hostname, MacBook-Pro-de-Thomas-2.local, resolves to a loopback address: 127.0.0.1; using 172.30.177.3 instead (on interface en0)\n",
      "25/11/19 11:42:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 11:42:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msql\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpyspark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mml\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfpm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FPGrowth\n\u001B[32m      4\u001B[39m spark = \u001B[43mSparkSession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuilder\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mFPGrowthExample\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlocal[*]\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mspark.driver.userClassPathFirst\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrue\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m fp = FPGrowth(\n\u001B[32m     12\u001B[39m     itemsCol=\u001B[33m\"\u001B[39m\u001B[33mitems\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     13\u001B[39m     minSupport=\u001B[32m0.6\u001B[39m,\n\u001B[32m     14\u001B[39m     minConfidence=\u001B[32m0.5\u001B[39m\n\u001B[32m     15\u001B[39m )\n\u001B[32m     17\u001B[39m model = fp.fit(df_2)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Personnel/Scolaire/Data Mining/TD Association/.venv/lib/python3.14/site-packages/pyspark/sql/session.py:556\u001B[39m, in \u001B[36mSparkSession.Builder.getOrCreate\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    554\u001B[39m     sparkConf.set(key, value)\n\u001B[32m    555\u001B[39m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m sc = \u001B[43mSparkContext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[32m    558\u001B[39m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[32m    559\u001B[39m session = SparkSession(sc, options=\u001B[38;5;28mself\u001B[39m._options)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Personnel/Scolaire/Data Mining/TD Association/.venv/lib/python3.14/site-packages/pyspark/core/context.py:523\u001B[39m, in \u001B[36mSparkContext.getOrCreate\u001B[39m\u001B[34m(cls, conf)\u001B[39m\n\u001B[32m    521\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext._lock:\n\u001B[32m    522\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m523\u001B[39m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    524\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext._active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    525\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext._active_spark_context\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Personnel/Scolaire/Data Mining/TD Association/.venv/lib/python3.14/site-packages/pyspark/core/context.py:207\u001B[39m, in \u001B[36mSparkContext.__init__\u001B[39m\u001B[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[39m\n\u001B[32m    205\u001B[39m SparkContext._ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway=gateway, conf=conf)\n\u001B[32m    206\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m207\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_do_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    208\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    209\u001B[39m \u001B[43m        \u001B[49m\u001B[43mappName\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    210\u001B[39m \u001B[43m        \u001B[49m\u001B[43msparkHome\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpyFiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    212\u001B[39m \u001B[43m        \u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    213\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatchSize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    215\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    216\u001B[39m \u001B[43m        \u001B[49m\u001B[43mjsc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprofiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[43mudf_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmemory_profiler_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    220\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m.stop()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Personnel/Scolaire/Data Mining/TD Association/.venv/lib/python3.14/site-packages/pyspark/core/context.py:300\u001B[39m, in \u001B[36mSparkContext._do_init\u001B[39m\u001B[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[39m\n\u001B[32m    297\u001B[39m \u001B[38;5;28mself\u001B[39m.environment[\u001B[33m\"\u001B[39m\u001B[33mPYTHONHASHSEED\u001B[39m\u001B[33m\"\u001B[39m] = os.environ.get(\u001B[33m\"\u001B[39m\u001B[33mPYTHONHASHSEED\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m0\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    299\u001B[39m \u001B[38;5;66;03m# Create the Java SparkContext through Py4J\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m300\u001B[39m \u001B[38;5;28mself\u001B[39m._jsc = jsc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_initialize_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conf\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_jconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    301\u001B[39m \u001B[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[39;00m\n\u001B[32m    302\u001B[39m \u001B[38;5;28mself\u001B[39m._conf = SparkConf(_jconf=\u001B[38;5;28mself\u001B[39m._jsc.sc().conf())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Personnel/Scolaire/Data Mining/TD Association/.venv/lib/python3.14/site-packages/pyspark/core/context.py:429\u001B[39m, in \u001B[36mSparkContext._initialize_context\u001B[39m\u001B[34m(self, jconf)\u001B[39m\n\u001B[32m    425\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    426\u001B[39m \u001B[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001B[39;00m\n\u001B[32m    427\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    428\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m._jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m429\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jvm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mJavaSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjconf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Personnel/Scolaire/Data Mining/TD Association/.venv/lib/python3.14/site-packages/py4j/java_gateway.py:1627\u001B[39m, in \u001B[36mJavaClass.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1621\u001B[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001B[32m   1622\u001B[39m     \u001B[38;5;28mself\u001B[39m._command_header +\\\n\u001B[32m   1623\u001B[39m     args_command +\\\n\u001B[32m   1624\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1626\u001B[39m answer = \u001B[38;5;28mself\u001B[39m._gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1627\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1628\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_gateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1630\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1631\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Personnel/Scolaire/Data Mining/TD Association/.venv/lib/python3.14/site-packages/py4j/protocol.py:327\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    325\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    328\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    329\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    330\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    332\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    333\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e46d14e897490344",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
